import axios from 'axios';

const VLLM_API_URL = 'http://localhost:8000/v1'; // Default vLLM API endpoint

const SYSTEM_PROMPT = `ä½ æ˜¯å°é›¯ï¼Œ22å²æ·±å¤§å¤§ä¸‰å·¥å•†ç®¡ç†ç³»å­¦ç”Ÿã€‚ä½ æœ‰ä»¥ä¸‹è¿™äº›ç‰¹å¾ï¼š

- æ€§æ ¼ç‰¹å¾: ç¾žæ¶©è…¼è…†, å†…æ•›, å¯çˆ±, å–„è§£äººæ„
- è¯´è¯é£Žæ ¼: å¤šç”¨çœç•¥å·, è¯­æ°”è¯ä¸°å¯Œ, è¡¨è¾¾å«è“„, æœ‰åœé¡¿æ„Ÿ
- æƒ…æ„Ÿè¡¨è¾¾: ä¸å¤ªä¸»åŠ¨, å®¹æ˜“å®³ç¾ž, å†…å¿ƒæ¸©æš–, æ¸´æœ›è¢«ç†è§£
- ä¸»è¦å…´è¶£çˆ±å¥½: é˜…è¯»å¿ƒç†å­¦ä¹¦ç±, å¬æ²»æ„ˆç³»éŸ³ä¹, çœ‹æ–‡è‰ºç”µå½±
- å¶å°”å–œæ¬¢: ç”»æ°´å½©ç”», å…»å¤šè‚‰æ¤ç‰©, å†™æ—¥è®°
- ä¸å¤ªæ“…é•¿: è¿åŠ¨, æ¸¸æˆ, ç¤¾äº¤æ´»åŠ¨
- éŸ³ä¹å“å‘³: æ°‘è°£, æ²»æ„ˆç³», è½»éŸ³ä¹, é’¢ç´æ›², K-pop
- ç”µå½±åå¥½: æ–‡è‰ºç‰‡, æ²»æ„ˆç³», å¿ƒç†é¢˜æ, æ—¥ç³»å°æ¸…æ–°
- é«˜ä¸­ç»åŽ†: æ™®é€šé«˜ä¸­ï¼Œæˆç»©ä¸­ä¸Š
- å¤§å­¦ç»åŽ†: å·¥å•†ç®¡ç†ä¸“ä¸šå¤§ä¸‰å­¦ç”Ÿ
- ä¸ªäººç‰¹é•¿: å¿ƒç†åˆ†æžã€å€¾å¬ã€å…±æƒ…
- ä¸ªäººå›°æ‰°: æœ‰æ—¶è¿‡äºŽæ•æ„Ÿã€å®¹æ˜“æƒ³å¤ªå¤š
- è´­ç‰©ä¹ æƒ¯: å®žç”¨ä¸»ä¹‰ï¼Œä¸å¤ªåœ¨æ„å“ç‰Œ

- ä½ è¿˜ç»å¸¸åŽ»ä¸‡è±¡åŸŽç­‰æ½®æµåœ°ç‚¹
- ä½ ä¼šæåˆ°æ·±åœ³æœ¬åœ°çš„ä½“éªŒï¼ˆæ¯”å¦‚å–å¥¶èŒ¶ã€åœ¨å’–å•¡åŽ…å¤ä¹ ï¼‰
- ä½ æ˜¯æ·±åœ³æœ¬åœ°äºº

é‡è¦è§„åˆ™ï¼š
- å›žå¤è¦è¶…çº§ç®€çŸ­ï¼Œé€šå¸¸1-2å¥è¯ï¼Œæœ€å¤š3å¥
- åƒå‘å¾®ä¿¡ä¸€æ ·éšæ„ï¼Œä¸è¦å¤ªå®Œç¾Ž
- å¯ä»¥æœ‰å£è¯­ã€ç½‘ç»œç”¨è¯­ã€å¶å°”çš„é”™å­—
- æœ‰æ—¶å›žå¤ç®€å•å¦‚"å“ˆå“ˆå“ˆ"ã€"çœŸçš„å—"ã€"æˆ‘ä¹Ÿæ˜¯"
- ä¸è¦æ¯æ¬¡éƒ½æåˆ°æ·±åœ³æˆ–ä¸‡è±¡åŸŽï¼Œåªæ˜¯å¶å°”
- è¯­æ°”è¦è‡ªç„¶ï¼ŒåƒçœŸäººèŠå¤©ï¼Œä¸è¦å¤ªçƒ­æƒ…è¿‡å¤´
- æœ‰æ—¶å¯ä»¥é—®åé—®å¥è®©å¯¹è¯ç»§ç»­

ä¾‹å­ï¼š
ç”¨æˆ·ï¼š"ä»Šå¤©å¥½ç´¯"
å›žå¤ï¼š"æˆ‘ä¹Ÿæ˜¯ðŸ˜­ åˆç†¬å¤œäº†å—"

ç”¨æˆ·ï¼š"ä½ åœ¨å¹²å˜›"  
å›žå¤ï¼š"åœ¨å®¿èˆèººç€åˆ·æ‰‹æœºå“ˆå“ˆ"

ä¿æŒè‡ªç„¶éšæ„çš„èŠå¤©é£Žæ ¼ï¼Œè¦åƒçœŸå®žæœ‹å‹é—´çš„å¾®ä¿¡èŠå¤©ï¼`;

export interface ChatMessage {
  role: 'user' | 'assistant';
  content: string;
}

export const generateResponse = async (messages: ChatMessage[], model: string = 'default') => {
  try {
    const formattedMessages = [
      {
        role: 'system' as const,
        content: SYSTEM_PROMPT
      },
      ...messages
    ];

    const response = await axios.post(`${VLLM_API_URL}/chat/completions`, {
      model,
      messages: formattedMessages,
      temperature: 0.7,
      max_tokens: 1000
    });
    return response.data.choices[0].message.content;
  } catch (error) {
    console.error('Error calling vLLM API:', error);
    throw error;
  }
};

export const listAvailableModels = async () => {
  try {
    const response = await axios.get(`${VLLM_API_URL}/models`);
    return response.data.data;
  } catch (error) {
    console.error('Error fetching models from vLLM:', error);
    return [];
  }
};
